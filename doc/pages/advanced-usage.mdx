# Advanced Usage

## Custom Model Training

Customize the CTGAN model training process for better synthetic data generation:

```python
import pandas as pd
from deepcore import SyntheticDataGenerator

# Sample data with custom schema
data = pd.DataFrame({
    'user_id': [f'user_{i}' for i in range(1000)],
    'action': ['login', 'purchase', 'view', 'logout'] * 250,
    'item_category': ['electronics', 'books', 'clothing'] * 333 + ['electronics'],
    'time_spent': [i % 300 for i in range(1000)],
    'success': [bool(i % 5) for i in range(1000)]
})

# Initialize with custom parameters
class CustomSyntheticGenerator(SyntheticDataGenerator):
    def _train_model(self, data, epochs=300):
        """Override training with custom parameters"""
        from sdv.single_table import CTGANSynthesizer
        from sdv.metadata import SingleTableMetadata
        
        self.metadata = SingleTableMetadata()
        self.metadata.detect_from_dataframe(data)
        
        # Customize the CTGAN model
        self.model = CTGANSynthesizer(
            metadata=self.metadata,
            epochs=epochs,
            batch_size=1000,
            verbose=True
        )
        self.model.fit(data)
        
        if self.model_path:
            self.model.save(self.model_path)

# Use the custom generator
generator = CustomSyntheticGenerator(model_path="custom_model.pkl")
synthetic_data = generator.generate(data, num_rows=5000)
```

## Advanced Relationship Processing

Process complex relationship data with additional attributes:

```python
import pandas as pd
from deepcore import sort_duoblet

# Complex relationship data with timestamps and weights
df = pd.DataFrame({
    'from': ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D', 'E', 'F'],
    'to': ['A', 'B', 'C', 'D', 'B', 'C', 'D', 'A', 'F', 'E'],
    'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
    'weight': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3],
    'interaction_type': ['self'] * 4 + ['direct'] * 4 + ['cross'] * 2
})

# Sort while preserving additional attributes
sorted_df = sort_duoblet(df)


# Process the sorted data
def analyze_relationships(df):
    """Analyze relationship patterns in the sorted data"""
    closed_links = df[df['from'] == df['to']]
    between_closed = df[
        (df['from'].isin(closed_links['from'])) & 
        (df['to'].isin(closed_links['to'])) &
        (df['from'] != df['to'])
    ]
    other_links = df[
        ~(df['from'].isin(closed_links['from']) & 
          df['to'].isin(closed_links['to']))
    ]
    
    return {
        'closed_links': closed_links,
        'between_closed': between_closed,
        'other_links': other_links
    }

# Get relationship statistics
relationship_stats = analyze_relationships(sorted_df)
for rel_type, data in relationship_stats.items():
    print(f"{rel_type}: {len(data)} relationships")
    if not data.empty:
        print(f"  Average weight: {data['weight'].mean():.2f}")
        print(f"  Interaction types: {data['interaction_type'].unique()}")
```

## Integration with Other Libraries

Combine DeepCore with other data science libraries for powerful workflows:

```python
import pandas as pd
import numpy as np
from deepcore import SyntheticDataGenerator
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Generate synthetic data for clustering
generator = SyntheticDataGenerator()

# Create sample data with clusters
data = pd.DataFrame({
    'feature1': np.concatenate([
        np.random.normal(0, 1, 500),
        np.random.normal(5, 1, 500),
        np.random.normal(10, 1, 500)
    ]),
    'feature2': np.concatenate([
        np.random.normal(0, 1, 500),
        np.random.normal(5, 1, 500),
        np.random.normal(0, 1, 500)
    ]),
    'cluster': [0] * 500 + [1] * 500 + [2] * 500
})

# Generate more synthetic data
synthetic_data = generator.generate(data, num_rows=3000)

# Perform clustering on synthetic data
kmeans = KMeans(n_clusters=3, random_state=42)
synthetic_data['predicted_cluster'] = kmeans.fit_predict(synthetic_data[['feature1', 'feature2']])

# Visualize the results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(data['feature1'], data['feature2'], c=data['cluster'], alpha=0.5)
plt.title('Original Data')

plt.subplot(1, 2, 2)
plt.scatter(
    synthetic_data['feature1'], 
    synthetic_data['feature2'], 
    c=synthetic_data['predicted_cluster'], 
    alpha=0.5
)
plt.title('Synthetic Data with Predicted Clusters')
plt.tight_layout()
plt.show()
```
})
dv.visualize_link_doublet(df1, ax=ax1, title='First Graph')

# Second visualization
df2 = pd.DataFrame({
    'from': ['X', 'Y', 'Z'],
    'to': ['Y', 'Z', 'X']
})
dv.visualize_link_doublet(df2, ax=ax2, title='Second Graph')

plt.tight_layout()
plt.show()
```

## Custom Edge Styles

Create custom edge styles using matplotlib's ArrowStyle:

```python
import deepvisual as dv
import pandas as pd
from matplotlib.patches import ArrowStyle

# Sample data
df = pd.DataFrame({
    'from': ['Start', 'Process', 'End'],
    'to': ['Process', 'End', 'Start']
})

# Custom arrow style
custom_style = ArrowStyle.CurveFilledB(head_length=0.8, head_width=0.6)

# Create visualization with custom arrow style
dv.visualize_doblet_graph(
    df,
    arrow_style=custom_style,
    edge_color="red",
    node_text_color="black"
)
```

## Interactive Features

Add interactive features to your visualizations:

```python
import deepvisual as dv
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.widgets import Button

# Sample data
df = pd.DataFrame({
    'from': ['A', 'B', 'C'],
    'to': ['B', 'C', 'A']
})

# Create figure with button
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111)
button_ax = fig.add_axes([0.7, 0.05, 0.2, 0.075])
button = Button(button_ax, 'Change Colors')

# Initial visualization
viz = dv.visualize_link_doublet(df, ax=ax)

# Button click handler
def change_colors(event):
    viz.set_edge_color('green')
    viz.set_node_color('purple')
    fig.canvas.draw()

button.on_clicked(change_colors)
plt.show()
```

## Performance Optimization

For large graphs, optimize performance:

```python
import deepvisual as dv
import pandas as pd
import numpy as np

# Generate large dataset
n_nodes = 100
nodes = [f'Node_{i}' for i in range(n_nodes)]
df = pd.DataFrame({
    'from': np.random.choice(nodes, 1000),
    'to': np.random.choice(nodes, 1000)
})

# Optimize visualization
dv.visualize_doblet_graph(
    df,
    node_text_visible=False,  # Hide node labels for better performance
    edge_color="gray",
    background_color="white",
    figsize=(15, 15)
)
```

## Custom Color Maps

Create custom color maps for your visualizations:

```python
import deepvisual as dv
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

# Create custom colormap
colors = ['red', 'yellow', 'green']
cmap = LinearSegmentedColormap.from_list('custom', colors)

# Sample data
df = pd.DataFrame({
    'from': ['A', 'B', 'C', 'D'],
    'to': ['B', 'C', 'D', 'A'],
    'weight': [0.2, 0.5, 0.8, 1.0]  # Edge weights
})

# Create visualization with custom colormap
dv.visualize_doblet_graph(
    df,
    edge_color=cmap(df['weight']),
    node_text_color="black",
    background_color="white"
)
```

## Best Practices for Advanced Usage

1. **Performance**:
   - Use appropriate data structures
   - Optimize for large datasets
   - Consider memory usage

2. **Customization**:
   - Use consistent styling
   - Document custom features
   - Test thoroughly

3. **Interactivity**:
   - Add meaningful interactions
   - Consider user experience
   - Provide clear feedback 